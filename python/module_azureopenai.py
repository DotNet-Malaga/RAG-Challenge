import os
import tiktoken
import statistics
import psycopg2
from psycopg2 import sql
import pandas as pd
from openai import AzureOpenAI
import datetime
from datetime import datetime


def get_chat_completion_from_azure_open_ai(system_message, user_prompt, temperature, tokens):
    """
    Retrieves a chat completion from Azure OpenAI API.
    Args:
        system_message (str): The system message.
        user_prompt (str): The user prompt.
        temperature (float): The temperature value for generating chat completions.
        tokens (int): The maximum number of tokens for generating chat completions.
    Returns:
        str: The generated chat completion.
    """
    client = AzureOpenAI(
        azure_endpoint=os.getenv('OPENAI_ENDPOINT'),
        api_key=os.getenv('OPENAI_KEY'),
        api_version="2023-05-15"
    )

    response = client.chat.completions.create(
        model=os.getenv('OPENAI_MODEL'),
        messages=[
            {"role": "system", "content": system_message},
            {"role": "user", "content": user_prompt},
        ],
        temperature=temperature,
        max_tokens=tokens,
    )

    output = response.choices[0].message.content

    output = output.replace('\n\n', '\n').replace('\n\n', '\n')

    return output


def get_tokens_statistics_from_table_column(source, table_name, column_name, filter, num_rows):
    """
    Retrieves statistics about the tokens in a specified table from a database.
    Args:
        table_name (str): The name of the table to retrieve data from.
        column_name (str): The name of the column to retrieve data from.
        num_rows (int): The maximum number of rows to retrieve. Set to 0 to retrieve all rows.
    Returns:
        dict: A dictionary containing the statistics of the tokens.
            - source (str): The source of the database. Either "azure" or "others".
            - table_name (str): The name of the table.
            - column_name (str): The name of the column.
            - total_rows (int): The total number of rows retrieved.
            - mean_tokens (float): The mean number of tokens per row.
            - median_tokens (float): The median number of tokens per row.
            - stddev_tokens (float): The standard deviation of the number of tokens per row.
    """
    encoder = tiktoken.get_encoding("cl100k_base")

    conn = None

    if source.lower() == "azure":
        # Connect to the Azure database
        conn = psycopg2.connect(
            host=os.getenv('DB_SERVER_AZURE'),
            database=os.getenv('DB_NAME_AZURE'),
            user=os.getenv('DB_USER_AZURE'),
            password=os.getenv('DB_PASSWORD_AZURE')
        )
    else:
        # Connect to the Azure database
        conn = psycopg2.connect(
            host=os.getenv('DB_SERVER'),
            database=os.getenv('DB_NAME'),
            user=os.getenv('DB_USER'),
            password=os.getenv('DB_PASSWORD')
        )

    sql = ""
    cursor = conn.cursor()

    sql = f"""
        select {column_name}
        from {table_name}
        """
    
    if filter:
        sql += f"where {filter} "

    if num_rows > 0:
        sql += f"limit {num_rows};"

    tokens_per_row = []
    cursor.execute(sql)

    for row in cursor:
        tokens = encoder.encode(row[0])
        tokens_per_row.append(len(tokens))
        if num_rows > 0 and len(tokens_per_row) >= num_rows:
            break
        if len(tokens_per_row) % 1000 == 0:
            print(".", end="")

    cursor.close()
    conn.close()
    print("")

    if tokens_per_row:
        mean_tokens = statistics.mean(tokens_per_row)
        median_tokens = statistics.median(tokens_per_row)
        stddev_tokens = statistics.stdev(tokens_per_row) if len(tokens_per_row) > 1 else 0
    else:
        mean_tokens = median_tokens = stddev_tokens = 0

    return {
        'table_name': table_name,
        'column_name': column_name,
        'total_rows': len(tokens_per_row),
        'mean_tokens': round(mean_tokens, 2),
        'median_tokens': round(median_tokens, 2),
        'stddev_tokens': round(stddev_tokens, 2)
    }


def create_and_download_detailed_match_summary(match_id, rows_per_prompt, file_prompt_size, temperature, system_message, tokens, local_folder):
    """
    Generate a summary from a given match ID. The summary is generated by calling the Azure OpenAI API.
    Args:
        match_id (str): The ID of the match.
        rows_per_prompt (int): The number of rows per prompt.
        file_prompt_size (int): The number of prompts per file.
        temperature (float): The temperature value for generating chat completions.
        system_message (str): The system message for generating chat completions.
        tokens (int): The maximum number of tokens for generating chat completions.
        local_folder (str): The local folder to save the generated files.
    Returns:
        None
    """
    start_time = datetime.now()

    num_files_in_batch = 0
    num_file = 0
    in_batch = True
    script = ""

    df = get_json_events_details_from_match_id(match_id)
    count = df.shape[0]

    total_num_files = count // (rows_per_prompt * file_prompt_size)
    if count % (rows_per_prompt * file_prompt_size) > 0:
        total_num_files += 1

    i = 0
    while i < count:
        batch_start_time = datetime.now()
        num_files_in_batch += 1

        df_batch = df.iloc[i:i+rows_per_prompt]
        user_prompt = df_batch.to_string(index=False)

        script += get_chat_completion_from_azure_open_ai(system_message, user_prompt, temperature, tokens)

        duration = datetime.now() - batch_start_time
        time_str = str(duration).split(".")[0]

        now = datetime.now()
        now_str = str(now).split(".")[0]

        print(f"[{now_str}] Batch processing time {num_files_in_batch}/{file_prompt_size}: {time_str} ", end="")

        time = ((datetime.now() - start_time) / (i+1)) * (count - i)
        time_str = str(time).split(".")[0]
        print(f"Estimated remaining time: {time_str}")

        if num_files_in_batch == file_prompt_size:
            num_files_in_batch = 0
            in_batch = False
            num_file += 1

            filename = f"{match_id}-{str(num_file).zfill(6)}-{str(total_num_files).zfill(6)}.txt"
            with open(os.path.join(local_folder, filename), "w", encoding="utf-8") as f:
                f.write(script)

            print(f"  Processed {i+1}/{count} rows. Generated file: {filename}. ")
            script = ""

        i += rows_per_prompt

    if in_batch:
        num_file += 1
        filename = f"{match_id}-{str(num_file).zfill(6)}-{str(total_num_files).zfill(6)}.txt"
        with open(os.path.join(local_folder, filename), "w", encoding="utf-8") as f:
            f.write(script)

        print(f"  Processed {i+1}/{count} rows. Generated file: {filename}. ")
        script = ""


def create_events_summary_per_pk_from_json_rows_in_database(source, tablename, primary_key_column, message_column, match_id, minute, system_message, temperature, tokens):
    """
    Converts JSON data to summary and updates the database with the generated summary.
    Args:
        source (str): The source of the database. Either "azure" or "others".
        tablename (str): The name of the table in the database.
        match_id (int): The ID of the match.
        pk (str): The primary key of the table.
        message_column (str): The name of the column to update.
        system_message (str): The system message.
        temperature (float): The temperature parameter for the OpenAI API.
        tokens (int): The maximum number of tokens for the OpenAI API.
    Returns:
        None: This function does not return any value.
    Raises:
        Exception: If there is an error connecting to or executing the query in the database.
    """
    try:

        conn = None

        if source.lower() == "azure":
            # Connect to the Azure database
            conn = psycopg2.connect(
                host=os.getenv('DB_SERVER_AZURE'),
                database=os.getenv('DB_NAME_AZURE'),
                user=os.getenv('DB_USER_AZURE'),
                password=os.getenv('DB_PASSWORD_AZURE')
            )
        else:
            # Connect to the Azure database
            conn = psycopg2.connect(
                host=os.getenv('DB_SERVER'),
                database=os.getenv('DB_NAME'),
                user=os.getenv('DB_USER'),
                password=os.getenv('DB_PASSWORD')
            )

        cursor = conn.cursor()

        query=""

        if minute >=0:
            query = sql.SQL(f"""
                SELECT  {primary_key_column} as key, json_ FROM {tablename}
                WHERE match_id = {match_id} 
                and {message_column} IS NULL 
                and minute = {minute}
                ORDER BY {primary_key_column};
            """)
        else:
            query = sql.SQL(f"""
                SELECT  {primary_key_column} as key, json_ FROM {tablename}
                WHERE match_id = {match_id} 
                and {message_column} IS NULL
                ORDER BY {primary_key_column};
            """)

        cursor.execute(query)
        rowCount = cursor.rowcount
        i = 0

        start_time = datetime.now()

        for row in cursor.fetchall():
            i += 1

            row_start_time = datetime.now()

            key = row[0]
            json_ = row[1]
            summary = get_chat_completion_from_azure_open_ai(system_message, json_, temperature, tokens)

            update_query = sql.SQL(f"""
                UPDATE {tablename}
                SET {message_column} = %s
                WHERE match_id = %s
                and {primary_key_column} = %s 
            """)

            cursor.execute(update_query, (summary, match_id, key))
            conn.commit()

            time = datetime.now() - row_start_time
            time_str = str(time).split(".")[0]

            now = datetime.now()
            now_str = str(now).split(".")[0]

            print(f"[{now_str}] Updated primary key {key}, {message_column} column, from match_id {match_id}.", end=" ")
            print(f"Row processing time {i} of {rowCount} row(s), {time_str}.", end=" ")

            time = ((datetime.now() - start_time) / (i+1)) * (rowCount - i)
            time_str = str(time).split(".")[0]

            # add time to now to get the estimated end time
            estimated_end = now + time
            estimated_end_str = str(estimated_end).split(".")[0]
           
            print(f"Estimated remaining time: {time_str} [{estimated_end_str}].")
            
    except Exception as e:
        print(f"Error connecting or executing the query in the database: {e}")

    finally:
        if cursor:
            cursor.close()
        if conn:
            conn.close()


def create_match_summary(source, tablename, match_id, system_message, temperature, tokens):
    """
    Retrieves the match summary from the specified database table for a given match ID by calling Azure Open AI for summarization.
    Args:
        source (str): The source of the database. Either "azure" or "others".
        tablename (str): The name of the table containing the match summaries.
        match_id (int): The ID of the match for which the summary is requested.
        system_message (str): The content to be used for generating the summary.
        temperature (float): The temperature parameter for generating the summary.
        tokens (int): The maximum number of tokens to use for generating the summary.
    Returns:
        str: The generated match summary.
    Raises:
        Exception: If there is an error connecting to the database or executing the query.
    """
    try:

        conn = None

        if source.lower() == "azure":
            # Connect to the Azure database
            conn = psycopg2.connect(
                host=os.getenv('DB_SERVER_AZURE'),
                database=os.getenv('DB_NAME_AZURE'),
                user=os.getenv('DB_USER_AZURE'),
                password=os.getenv('DB_PASSWORD_AZURE')
            )
        else:
            # Connect to the Azure database
            conn = psycopg2.connect(
                host=os.getenv('DB_SERVER'),
                database=os.getenv('DB_NAME'),
                user=os.getenv('DB_USER'),
                password=os.getenv('DB_PASSWORD')
            )

        cursor = conn.cursor()

        query = sql.SQL(f"""
            SELECT summary FROM {tablename}
            WHERE match_id = {match_id} 
            ORDER BY period, minute
        """)
        cursor.execute(query)
        rowCount = cursor.rowcount
        i = 0

        all_text = ""
        for row in cursor.fetchall():
            all_text += row[0]

        summary = get_chat_completion_from_azure_open_ai(system_message, all_text, temperature, tokens)

        return summary

    except Exception as e:
        print(f"Error connecting or executing the query in the database: {e}")

    finally:
        if cursor:
            cursor.close()
        if conn:
            conn.close()


def search_details_using_bindings(source, table_name, match_id, team_name, search_term, system_message, temperature, tokens):

    try:

        conn = None

        if source.lower() == "azure":
            # Connect to the Azure database
            conn = psycopg2.connect(
                host=os.getenv('DB_SERVER_AZURE'),
                database=os.getenv('DB_NAME_AZURE'),
                user=os.getenv('DB_USER_AZURE'),
                password=os.getenv('DB_PASSWORD_AZURE')
            )
        else:
            # Connect to the Azure database
            conn = psycopg2.connect(
                host=os.getenv('DB_SERVER'),
                database=os.getenv('DB_NAME'),
                user=os.getenv('DB_USER'),
                password=os.getenv('DB_PASSWORD')
            )

        cursor = conn.cursor()

        query = sql.SQL(f"""
            -- Retrieve similarities. NIP
            SELECT 
                ed.id, ed.period, ed.minute, ed.summary,
                m.match_date, m.competition_name, m.season_name, m.home_team_name, m.away_team_name, m.result
            FROM matches m
            JOIN {table_name} ed on m.match_id = ed.match_id
            WHERE home_team_name = '{team_name}' or away_team_name = '{team_name}'
            and ed.match_id = {match_id}
            ORDER BY ed.summary_embedding_t3_small <#> azure_openai.create_embeddings('text-embedding-3-small', '{search_term}')::vector
            LIMIT 10;
        """)

        cursor.execute(query)
        rowCount = cursor.rowcount

        if rowCount == 0:
            return "No results found."

        # convertir el resultado a un pandas dataframe
        df1 = pd.DataFrame(cursor.fetchall(), columns=[desc[0] for desc in cursor.description])

        # obtener array de los identificadores de la primera columna
        ids = df1['id'].values
        extended_ids = []

        # para cada numero en ids
        for i in ids:
            extended_ids.append(i-1)
            extended_ids.append(i+1)

        # remove duplicates and order by id
        ids = list(set(extended_ids))
        ids.sort()

        # añadir al df la llamada a una función que recupere los identificadores
        df2 = get_dataframe_from_ids(source, table_name, ids)

        df1['match_date'] = df1['match_date'].astype('datetime64[ns]')
        df2['match_date'] = df2['match_date'].astype('datetime64[ns]')

        # Concatenar los dataframes
        df = pd.concat([df1, df2], ignore_index=True)
        df = df.drop_duplicates(subset='id', keep='first')
        df = df.sort_values(by='id')

        result = df.to_string(index=False)

        summary = get_chat_completion_from_azure_open_ai(system_message, result, temperature, tokens)

        return summary

    except Exception as e:
        print(f"Error connecting or executing the query in the database: {e}")

    finally:
        if cursor:
            cursor.close()
        if conn:
            conn.close()

def get_dataframe_from_ids(source, table_name, ids):

    try:

        conn = None

        if source.lower() == "azure":
            # Connect to the Azure database
            conn = psycopg2.connect(
                host=os.getenv('DB_SERVER_AZURE'),
                database=os.getenv('DB_NAME_AZURE'),
                user=os.getenv('DB_USER_AZURE'),
                password=os.getenv('DB_PASSWORD_AZURE')
            )
        else:
            # Connect to the Azure database
            conn = psycopg2.connect(
                host=os.getenv('DB_SERVER'),
                database=os.getenv('DB_NAME'),
                user=os.getenv('DB_USER'),
                password=os.getenv('DB_PASSWORD')
            )

        cursor = conn.cursor()

        # convertir array de ids a string de numeros separado por comas
        ids_str = ','.join(map(str, ids))

        query = sql.SQL(f"""
            -- Retrieve similarities. NIP
            SELECT 
                ed.id, ed.period, ed.minute, ed.summary,
                m.match_date, m.competition_name, m.season_name, m.home_team_name, m.away_team_name, m.result
            FROM matches m
            JOIN {table_name} ed on m.match_id = ed.match_id
            WHERE ed.id IN ({ids_str});
        """)
        cursor.execute(query)

        # convertir el resultado a un pandas dataframe
        df = pd.DataFrame(cursor.fetchall(), columns=[desc[0] for desc in cursor.description])
        return df

    except Exception as e:
        print(f"Error connecting or executing the query in the database: {e}")

    finally:
        if cursor:
            cursor.close()
        if conn:
            conn.close()
